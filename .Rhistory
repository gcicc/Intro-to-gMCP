0, 1, 0, 0, 0, 0,
0, 0, .5, .5, 0, 0,
0, 0, 0, 1, 0, 0,
0, 0, 0, 0, .5, .5,
0, 0, 0, 0, 0, 1,
.5, .5, 0, 0, 0, 0
), nrow = 6, byrow = TRUE)
# Initial Type I error assigned to each hypothesis (one-sided)
alphaHypotheses <- c(.01, .01, .004, 0.000, 0.0005, .0005)
fwer <- sum(alphaHypotheses)
# Make a ggplot representation of the above specification and display it
g <- gMCPLite::hGraph(6,
alphaHypotheses = alphaHypotheses, m = m, nameHypotheses = nameHypotheses,
palette = cbPalette,
halfWid = 1, halfHgt = .35, xradius = 2.5, yradius = 1, offset = 0, trhw = .15,
x = c(-1.25, 1.25, -2.5, 2.5, -1.25, 1.25), y = c(2, 2, 1, 1, 0, 0),
trprop = 0.4, fill = as.character(c(2, 2, 4, 4, 3, 3))
)
print(g)
# Chunk 3
osmedian <- 12 # Median control survival
# Derive group sequential design for OS in the targeted subgroup
ossub <- gsDesign::gsSurv(
k = 3, # 3 analyses for OS
test.type = 1, # Efficacy bound only (no futility)
alpha = alphaHypotheses[1], # Allocated alpha from design hypothesis group
beta = 0.1, # Type 2 error (1 - power)
hr = 0.65, # Assumed hazard ratio for power calculation
timing = c(0.61, 0.82), # Choose these to match targeted calendar timing of analyses
sfu = sfLDOF, # Spending function to approximate O'Brien-Fleming bound
lambdaC = log(2) / osmedian, # Exponential control failure rate
eta = 0.001, # Exponential dropout rate
gamma = c(2.5, 5, 7.5, 10), # Relative enrollment rates by time period
R = c(2, 2, 2, 12), # Duration of time periods for rates in gamma
T = 42, # Planned study duration for OS
minfup = 24 # Planned minimum follow-up after end of enrollment
)
tab <- gsDesign::gsBoundSummary(ossub)
rownames(tab) <- 1:nrow(tab)
cat(summary(ossub))
# Chunk 4
tab %>%
gt() %>%
tab_header(title = "Design for OS in the Subgroup") %>%
cols_align(align = "left", columns = Value) %>%
tab_footnote(
footnote = "Cumulative boundary crossing probability includes crossing probability at earlier analysis.",
locations = cells_body(columns = "Value", rows = c(9, 10, 14, 15))
) %>%
tab_footnote(
footnote = "Approximate hazard ratio at bound.",
locations = cells_body(columns = "Value", rows = c(3, 8, 13))
)
# Chunk 5
hr <- .75
beta <- .14
os <- gsDesign::gsSurv(
k = 3, test.type = 4, alpha = 0.01, beta = beta, hr = hr,
timing = c(0.62, 0.83), sfu = sfLDOF,
sfl = sfHSD, sflpar = -3.25,
lambdaC = log(2) / 12, eta = 0.001, S = NULL,
gamma = c(2.5, 5, 7.5, 10), R = c(2, 2, 2, 12),
T = 42, minfup = 24
)
tab <- gsDesign::gsBoundSummary(os)
rownames(tab) <- 1:nrow(tab)
cat(summary(os))
# Chunk 6
tab %>%
kable(caption = "Design for OS in all subjects") %>%
kable_styling()
# Chunk 7
plot(os, plottype = "HR", xlab = "Events")
# Chunk 8
hr <- .65
beta <- .149
pfssub <- gsDesign::gsSurv(
k = 2, test.type = 6, astar = 0.1, alpha = 0.004, beta = beta, hr = hr,
timing = .87, sfu = sfLDOF,
sfl = sfHSD, sflpar = -8,
lambdaC = log(2) / 5, eta = 0.02, S = NULL,
gamma = c(2.5, 5, 7.5, 10), R = c(2, 2, 2, 12),
T = 32, minfup = 14
)
tab <- gsDesign::gsBoundSummary(pfssub)
rownames(tab) <- 1:nrow(tab)
cat(summary(pfssub))
# Chunk 9
tab %>%
kable(caption = "Design for PFS in the subgroup") %>%
kable_styling()
# Chunk 10
hr <- .74
beta <- .15
pfs <- gsDesign::gsSurv(
k = 2, test.type = 1, alpha = 0.004, beta = beta, hr = hr,
timing = .86, sfu = sfLDOF,
lambdaC = log(2) / 5, eta = 0.02, S = NULL,
gamma = c(2.5, 5, 7.5, 10), R = c(2, 2, 2, 12),
T = 32, minfup = 14
)
tab <- gsDesign::gsBoundSummary(pfs)
rownames(tab) <- 1:nrow(tab)
tab %>%
kable(caption = "Design for PFS in the overall population") %>%
kable_styling()
# Chunk 11
nBinomial(p1 = .35, p2 = .15, alpha = .0005, n = 378)
# Chunk 12
nBinomial(p1 = .3, p2 = .15, alpha = .0005, n = 756)
# Chunk 13
gsDlist <- list(ossub, os, pfssub, pfs, NULL, NULL)
# Chunk 14
### THIS NEEDS TO BE MODIFIED TO MATCH YOUR STUDY
# PFS, overall population
pfs$n.I <- c(675, 750)
# PFS, subgroup
pfssub$n.I <- c(265, 310)
# OS, overall population
os$n.I <- c(529, 700, 800)
# OS, subgroup
ossub$n.I <- c(185, 245, 295)
# Chunk 15
### THIS NEEDS TO BE MODIFIED TO MATCH YOUR STUDY
inputResults <- tibble(
H = c(rep(1, 3), rep(2, 3), rep(3, 2), rep(4, 2), 5, 6),
Pop = c(
rep("Subgroup", 3), rep("All", 3),
rep("Subgroup", 2), rep("All", 2),
"Subgroup", "All"
),
Endpoint = c(rep("OS", 6), rep("PFS", 4), rep("ORR", 2)),
# Example with some rejections
nominalP = c(
.03, .0001, .000001,
.2, .15, .1,
.2, .001,
.3, .2,
.00001,
.1
),
# Example with no rejections
# nominalP = rep(.03, 12),
Analysis = c(1:3, 1:3, 1:2, 1:2, 1, 1),
events = c(ossub$n.I, os$n.I, pfssub$n.I, pfs$n.I, NA, NA),
spendingTime = c(
ossub$n.I / max(ossub$n.I),
ossub$n.I / max(ossub$n.I),
pfssub$n.I / max(pfssub$n.I),
pfssub$n.I / max(pfssub$n.I),
NA, NA
)
)
kable(inputResults, caption = "DUMMY RESULTS FOR IA2.") %>%
kable_styling() %>%
add_footnote("Dummy results", notation = "none")
# Chunk 16
### USER SHOULD NOT NEED TO MODIFY THIS CODE
EOCtab <- NULL
EOCtab <- inputResults %>%
group_by(H) %>%
slice(1) %>%
ungroup() %>%
select("H", "Pop", "Endpoint", "nominalP")
EOCtab$seqp <- .9999
for (EOCtabline in 1:nHypotheses) {
EOCtab$seqp[EOCtabline] <-
ifelse(is.null(gsDlist[[EOCtabline]]), EOCtab$nominalP[EOCtabline], {
tem <- filter(inputResults, H == EOCtabline)
sequentialPValue(
gsD = gsDlist[[EOCtabline]], interval = c(.0001, .9999),
n.I = tem$events,
Z = -qnorm(tem$nominalP),
usTime = tem$spendingTime
)
})
}
EOCtab <- EOCtab %>% select(-"nominalP")
# kable(EOCtab,caption="Sequential p-values as initially placed in EOCtab") %>% kable_styling()
# Chunk 17
# Make a graph object
rownames(m) <- nameHypotheses
graph <- matrix2graph(m)
# Add weights to the object based on alpha allocation
graph <- setWeights(graph, alphaHypotheses / fwer)
rescale <- 45
d <- g$layers[[2]]$data
rownames(d) <- rownames(m)
# graph@nodeAttr$X <- rescale * d$x * 1.75
# graph@nodeAttr$Y <- -rescale * d$y * 2
# Chunk 18
result <- gMCP(graph = graph, pvalues = EOCtab$seqp, alpha = fwer)
result@rejected
# Chunk 19
# Number of graphs is used repeatedly
ngraphs <- length(result@graphs)
# Set up tibble with hypotheses rejected at each stage
rejected <- NULL
for (i in 1:length(result@graphs)) {
rejected <- rbind(
rejected,
tibble(
H = 1:nHypotheses, Stage = i,
Rejected = as.logical(result@graphs[[i]]@nodeAttr$rejected)
)
)
}
rejected <- rejected %>%
filter(Rejected) %>%
group_by(H) %>%
summarize(graphRejecting = min(Stage) - 1, .groups = "drop") %>% # Last graph with weight>0 where H rejected
arrange(graphRejecting)
# Get final weights
# for hypotheses not rejected, this will be final weight where
# no hypothesis could be rejected
lastWeights <- as.numeric(result@graphs[[ngraphs]]@weights)
lastGraph <- rep(ngraphs, nrow(EOCtab))
# We will update for rejected hypotheses with last positive weight for each
if (ngraphs > 1) {
for (i in 1:(ngraphs - 1)) {
lastWeights[rejected$H[i]] <- as.numeric(result@graphs[[i]]@weights[rejected$H[i]])
lastGraph[rejected$H[i]] <- i
}
}
EOCtab$lastAlpha <- fwer * lastWeights
EOCtab$lastGraph <- lastGraph
EOCtabx <- EOCtab
names(EOCtabx) <- c(
"Hypothesis", "Population", "Endpoint", "Sequential p",
"Rejected", "Adjusted p", "Max alpha allocated", "Last Graph"
)
# Display table with desired column order
# Delayed following until after multiplicity graph sequence
# EOCtabx %>% select(c(1:4,7,5:6,8)) %>% kable() %>% kable_styling()
EOCtabx %>%
select(c(1:4, 7, 5:6, 8)) %>%
kable() %>%
kable_styling()
fwer
tab %>%
gt() %>%
tab_header(title = "Design for OS in the Subgroup") %>%
cols_align(align = "left", columns = Value) %>%
tab_footnote(
footnote = "Cumulative boundary crossing probability includes crossing probability at earlier analysis.",
locations = cells_body(columns = "Value", rows = c(9, 10, 14, 15))
) %>%
tab_footnote(
footnote = "Approximate hazard ratio at bound.",
locations = cells_body(columns = "Value", rows = c(3, 8, 13))
)
# Chunk 1
### THERE SHOULD BE NO NEED TO MODIFY THIS CODE SECTION
# Prefer fixed notation
old <- options(scipen = 999)
# Colorblind palette
cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
# 2 packages used for data storage and manipulation: dplyr, tibble
library(dplyr)
library(tibble)
# 2 packages used for R Markdown capabilities: knitr, kableExtra
library(knitr)
library(kableExtra)
library(gt)
library(ggplot2) # For plotting
library(gsDesign) # Group sequential design capabilities
library(gMCPLite) # Multiplicity evaluation
# Chunk 2
### THIS CODE NEEDS TO BE MODIFIED FOR YOUR STUDY
# If needed, see help file for gMCPLite::hGraph() for explanation of parameters below
# Hypothesis names
nameHypotheses <- c(
"H1: OS\n Subgroup",
"H2: OS\n All subjects",
"H3: PFS\n Subgroup",
"H4: PFS\n All subjects",
"H5: ORR\n Subgroup",
"H6: ORR\n All subjects"
)
# Number of hypotheses to be tested
nHypotheses <- length(nameHypotheses)
# Transition weights for alpha reallocation (square matrix)
m <- matrix(c(
0, 1, 0, 0, 0, 0,
0, 0, .5, .5, 0, 0,
0, 0, 0, 1, 0, 0,
0, 0, 0, 0, .5, .5,
0, 0, 0, 0, 0, 1,
.5, .5, 0, 0, 0, 0
), nrow = 6, byrow = TRUE)
# Initial Type I error assigned to each hypothesis (one-sided)
alphaHypotheses <- c(.01, .01, .004, 0.000, 0.0005, .0005)
fwer <- sum(alphaHypotheses)
# Make a ggplot representation of the above specification and display it
g <- gMCPLite::hGraph(6,
alphaHypotheses = alphaHypotheses, m = m, nameHypotheses = nameHypotheses,
palette = cbPalette,
halfWid = 1, halfHgt = .35, xradius = 2.5, yradius = 1, offset = 0, trhw = .15,
x = c(-1.25, 1.25, -2.5, 2.5, -1.25, 1.25), y = c(2, 2, 1, 1, 0, 0),
trprop = 0.4, fill = as.character(c(2, 2, 4, 4, 3, 3))
)
print(g)
# Chunk 3
osmedian <- 12 # Median control survival
# Derive group sequential design for OS in the targeted subgroup
ossub <- gsDesign::gsSurv(
k = 3, # 3 analyses for OS
test.type = 1, # Efficacy bound only (no futility)
alpha = alphaHypotheses[1], # Allocated alpha from design hypothesis group
beta = 0.1, # Type 2 error (1 - power)
hr = 0.65, # Assumed hazard ratio for power calculation
timing = c(0.61, 0.82), # Choose these to match targeted calendar timing of analyses
sfu = sfLDOF, # Spending function to approximate O'Brien-Fleming bound
lambdaC = log(2) / osmedian, # Exponential control failure rate
eta = 0.001, # Exponential dropout rate
gamma = c(2.5, 5, 7.5, 10), # Relative enrollment rates by time period
R = c(2, 2, 2, 12), # Duration of time periods for rates in gamma
T = 42, # Planned study duration for OS
minfup = 24 # Planned minimum follow-up after end of enrollment
)
tab <- gsDesign::gsBoundSummary(ossub)
rownames(tab) <- 1:nrow(tab)
cat(summary(ossub))
tab %>%
gt() %>%
tab_header(title = "Design for OS in the Subgroup") %>%
cols_align(align = "left", columns = Value) %>%
tab_footnote(
footnote = "Cumulative boundary crossing probability includes crossing probability at earlier analysis.",
locations = cells_body(columns = "Value", rows = c(9, 10, 14, 15))
) %>%
tab_footnote(
footnote = "Approximate hazard ratio at bound.",
locations = cells_body(columns = "Value", rows = c(3, 8, 13))
)
hr <- .65
beta <- .149
pfssub <- gsDesign::gsSurv(
k = 2, test.type = 6, astar = 0.1, alpha = 0.004, beta = beta, hr = hr,
timing = .87, sfu = sfLDOF,
sfl = sfHSD, sflpar = -8,
lambdaC = log(2) / 5, eta = 0.02, S = NULL,
gamma = c(2.5, 5, 7.5, 10), R = c(2, 2, 2, 12),
T = 32, minfup = 14
)
tab <- gsDesign::gsBoundSummary(pfssub)
rownames(tab) <- 1:nrow(tab)
cat(summary(pfssub))
I will give you some text. You will summarize as if for a powerpoint presentation. I will tell you how many slides to produce.  You will start a new slide by prefacing the slide's title with: ##.  You will use bullets and subbullets with number of these per slide limited to 12 at the most.  I want to paste this into a markdown document. Please use unformatted text.  A slide should look like this:
I will give you some text. You will summarize as if for a powerpoint presentation. I will tell you how many slides to produce.  You will start a new slide by prefacing the slide's title with: ##.  You will use bullets and subbullets with number of these per slide limited to 12 at the most.  I want to paste this into a markdown document. Please use unformatted text.  A slide should look like this:
- Helps bridge the gap between user's knowledge and LLM's understanding
- LLM can produce factual inaccuracies
x <- scan()
table(x)
require(gpackage)
gcurve(expr = dbeta(x, shape1 = 1, shape2 = 1),from = 0, to = 1, category = "beta1 - unif")
gcurve(expr = dbeta(x, shape1 = 8.5, shape2 = 1.5),from = 0, to = 1, category = "beta2 - week 26")) %>% ggplot(aes(x=x, y=y,color=category)) + geom_line()
bind_rows(
gcurve(expr = dbeta(x, shape1 = 1, shape2 = 1),from = 0, to = 1, category = "beta1 - week 4")
gcurve(expr = dbeta(x, shape1 = 8.5, shape2 = 1.5),from = 0, to = 1, category = "beta2 - week 26"))
bind_rows(
gcurve(expr = dbeta(x, shape1 = 1, shape2 = 1),from = 0, to = 1, category = "beta1 - week 4"),
gcurve(expr = dbeta(x, shape1 = 8.5, shape2 = 1.5),from = 0, to = 1, category = "beta2 - week 26"))
bind_rows(
gcurve(expr = dbeta(x, shape1 = 1, shape2 = 1),from = 0, to = 1, category = "beta1 - week 4"),
gcurve(expr = dbeta(x, shape1 = 8.5, shape2 = 1.5),from = 0, to = 1, category = "beta2 - week 26")) %>% ggplot(aes(x=x, y=y,color=category)) + geom_line()
bind_rows(
gcurve(expr = dbeta(x, shape1 = 1, shape2 = 1),from = 0, to = 1, category = "beta1 - week 4"),
gcurve(expr = dbeta(x, shape1 = .01, shape2 = 999.99),from = 0, to = 1, category = "beta3"),
gcurve(expr = dbeta(x, shape1 = 8.5, shape2 = 1.5),from = 0, to = 1, category = "beta2 - week 26")) %>% ggplot(aes(x=x, y=y,color=category)) + geom_line()
require(tidyverse)
require(broom)
require(tidyverse)
my.df <- expand.grid(Power=seq(0,1,.01), POS = seq(0,1,.01)) %>%
filter(Power >= POS) %>%
filter(POS >= 0.5) %>%
mutate(ratio=Power/POS)
my.df %>% ggplot(aes(x=Power,y=POS, fill=ratio)) + geom_tile()+
scale_fill_gradientn(colours = terrain.colors(10)) + geom_vline(xintercept = c(.89, .91))
power.prop.test(n = 100, p1 = .5, p2=0.73, alternative = "one.sided", sig.level = .025)
Power <- function(n=100, p1 = .5, p2 = .73){
s1 <- rbinom(n = 1, size = n, prob = p1)
s2 <- rbinom(n = 1, size = n, prob = p2)
tidy(prop.test(x = c(s1, s2), n = c(100, 100), alternative = "less", correct=TRUE))
}
POS <- function(n=100, C1=.5, epsilon1 = .05, C2 = .73, epsilon2 = .05){
sample1 <- runif(n = 10000, min= C1 - epsilon1, max = C1 + epsilon1)
sample2 <- runif(n = 10000, min= C2 - epsilon2, max = C2 + epsilon2)
bind_rows(
apply(matrix(1:10000), MARGIN = 1, FUN = function(x){
tidy(prop.test(x = c(rbinom(n = 1, size = 100, prob = sample1[x]),
rbinom(n = 1, size = 100, prob = sample2[x])),
n=c(100,100),
alternative="less")) %>%
mutate(C1=C1, epsilon1=epsilon1, C2=C2, epsilon2=epsilon2, ssize=n)
}
))%>%
group_by(C1, C2, epsilon1, epsilon2) %>%
summarize(POS = mean(p.value < .025, na.rm=T))
}
tictoc::tic()
POS()
tictoc::toc()
my.grid = expand.grid(epsilon1 = seq(0,.5,.02), epsilon2=seq(0,.27,.015))
gpackage::goparallel(ncores = 12)
require(parallel)
help(stats::p.adjust)
#| message: false
require(tidyverse)
library(readr)
require(broom)
require(parallel)
require(gridExtra)
require(gMCP)
require(gt)
#| message: false
goparallel <- function(ncores = 7)
{
cat(paste("\nCurrent Connections: ", dim(showConnections())[1], "\n"))
cat("\nClosing any open connections...\n")
#closeAllConnections()
if (exists("cl"))
remove(cl)
cat(paste("\nCurrent Connections: ", dim(showConnections())[1], "\n"))
cat(paste("\nStarting new cluster with", ncores, "cores...\n"))
cl <<- parallel::makeCluster(spec = ncores, type = "PSOCK")
cat(" Cluster initiation complete\n")
cat(paste("\nCurrent Connections: ", dim(showConnections())[1], "\n"))
cat(paste("\n", exists("cl"), "\n"))
parallel::clusterEvalQ(cl = cl, expr = {
require(tidyverse)
})
cat(
"\n\n***\nThe tidyverse pacakge has been sent to each core.\nDo you need other
parallel::clusterEvalQ or parallel::clusterExport calls before running your code?\n****\n"
)
}
#| message: false
eyecare_data <- read_csv("data/eyecare-data.csv")
require(here)
require(here)
#| message: false
eyecare_data <- read_csv(
here("data", "eyecare-data.csv"))
eyecare_data %>%
filter(SUBJID == unique(SUBJID)[1]) %>%
arrange(Hyp.No) %>%
select(Hyp.No, everything(),-SUBJID) %>%
gt()
for.inspection %>% select(-NCP2) %>% sample_n(15) %>% arrange(NCP) %>%
gt()
my.NCP
result.Bonferroni
temp.Bonferroni
temp.Bonferroni <-
calcPower(
graph = graph.Bonferroni,
mean = my.short.list,
f = f,
type = "quasirandom",
corr.sim = corr.sim,
alpha = 0.025,
n.sim = 5000  )
temp.Bonferroni
result.Bonferroni
temp.Bonferroni <-
calcPower(
graph = graph.Bonferroni,
mean = my.NCP,
f = f,
type = "quasirandom",
corr.sim = corr.sim,
alpha = 0.025,
n.sim = 5000  )
data.frame(t(
apply(X = matrix(1:length(temp.Bonferroni)), MARGIN = 1,
function(x)
temp.Bonferroni[[x]]$LocalPower)  )) %>%
data.frame(NCP = NCP.values, case = "Bonferroni")
temp.Bonferroni <-
calcPower(
graph = graph.Bonferroni,
mean = my.NCP,
f = f,
type = "quasirandom",
corr.sim = corr.sim,
alpha = 0.025,
n.sim = 5000  )
result.Bonferroni <-
data.frame(t(
apply(X = matrix(1:length(temp.Bonferroni)), MARGIN = 1,
function(x)
temp.Bonferroni[[x]]$LocalPower)  )) %>%
data.frame(NCP = NCP.values, case = "Bonferroni")
temp.Bonferroni
length(temp.Bonferroni)
temp.Bonferroni[[x]]$LocalPower
temp.Bonferroni
my.NCP
temp.Bonferroni <-
calcPower(
graph = graph.Bonferroni,
mean = my.NCP,
f = f,
type = "quasirandom",
corr.sim = corr.sim,
alpha = 0.025,
n.sim = 5000  )
temp.Bonferroni
require(gMCP)
require(gMCP)
require(gMCP)
